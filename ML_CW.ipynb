{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HRashmika/Bank_Prediction/blob/main/ML_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZz5aUWV3KHu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import os\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# File path\n",
        "file_path = 'bank-additional-full.csv'\n",
        "data_add = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Debugging: print the original data\n",
        "print(\"Original DataFrame Shape:\", data_add.shape)\n",
        "\n",
        "# Step 1: Remove the 'duration' column\n",
        "data_add = data_add.drop('duration', axis=1, errors='ignore')\n",
        "\n",
        "# Step 2: One-hot encode categorical columns\n",
        "one_hot_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan',\n",
        "                   'contact', 'month', 'day_of_week']\n",
        "data_add = pd.get_dummies(data_add, columns=one_hot_columns, drop_first=True)\n",
        "\n",
        "# Step 3: Label encode 'poutcome' column\n",
        "label_columns = ['poutcome']\n",
        "label_encoder = LabelEncoder()\n",
        "label_mappings = {}\n",
        "\n",
        "for col in label_columns:\n",
        "    data_add[col] = label_encoder.fit_transform(data_add[col].fillna('unknown'))  # Fill NaNs with 'unknown'\n",
        "    label_mappings[col] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "# Encode target column 'y'\n",
        "data_add['y'] = data_add['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Print label mappings\n",
        "print(\"\\nLabel Mappings for 'poutcome':\", label_mappings)\n",
        "\n",
        "# Step 4: Handle duplicates\n",
        "duplicates = data_add.duplicated().sum()\n",
        "if duplicates > 0:\n",
        "    print(f\"\\n{duplicates} duplicate rows found and removed.\")\n",
        "    data_add = data_add.drop_duplicates()\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "# Step 5: Handle missing values\n",
        "missing_values = data_add.isnull().sum()\n",
        "if missing_values.any():\n",
        "    print(\"\\nMissing Values Found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "    for col in data_add.columns:\n",
        "        if data_add[col].dtype == 'object':\n",
        "            data_add[col].fillna('unknown', inplace=True)\n",
        "        else:\n",
        "            data_add[col].fillna(data_add[col].median(), inplace=True)\n",
        "else:\n",
        "    print(\"\\nNo missing values found.\")\n",
        "\n",
        "# Step 6: Min-Max Normalization\n",
        "scaler = MinMaxScaler()\n",
        "features_to_scale = data_add.drop('y', axis=1)\n",
        "data_add[features_to_scale.columns] = scaler.fit_transform(features_to_scale)\n",
        "\n",
        "# Step 7: Split data into features (X) and target (y)\n",
        "X = data_add.drop('y', axis=1)\n",
        "y = data_add['y']\n",
        "\n",
        "# Step 8: Train-test split (80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 9: Balance the training dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Combine balanced data for inspection\n",
        "train_data_add = pd.DataFrame(X_train_balanced, columns=X.columns)\n",
        "train_data_add['y'] = y_train_balanced\n",
        "\n",
        "test_data_add = X_test.copy()\n",
        "test_data_add['y'] = y_test\n",
        "\n",
        "# Print dataset shapes and class distributions\n",
        "print(\"\\nTraining Data Shape (After Balancing):\", train_data_add.shape)\n",
        "print(\"Testing Data Shape:\", test_data_add.shape)\n",
        "print(\"\\nClass Distribution in Balanced Training Data:\")\n",
        "print(train_data_add['y'].value_counts(normalize=True))\n",
        "\n",
        "# Final DataFrame Info\n",
        "print(\"\\nFinal DataFrame Info:\")\n",
        "data_add.info()\n",
        "print(\"\\nFinal DataFrame Preview:\")\n",
        "print(data_add.head())\n",
        "\n",
        "# Check class distribution in the target variable ('y')\n",
        "class_distribution = data_add['y'].value_counts(normalize=True) * 100\n",
        "print(\"Class Distribution (%):\")\n",
        "print(class_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efsQjCUFXYHN",
        "outputId": "f6061b3d-9349-4564-de3d-d5a219916dc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame Shape: (41188, 21)\n",
            "\n",
            "Label Mappings for 'poutcome': {'poutcome': {'failure': 0, 'nonexistent': 1, 'success': 2}}\n",
            "\n",
            "1784 duplicate rows found and removed.\n",
            "\n",
            "No missing values found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7937df36c96c>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_add[features_to_scale.columns] = scaler.fit_transform(features_to_scale)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data Shape (After Balancing): (55690, 52)\n",
            "Testing Data Shape: (7881, 52)\n",
            "\n",
            "Class Distribution in Balanced Training Data:\n",
            "y\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Final DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 39404 entries, 0 to 41187\n",
            "Data columns (total 52 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   age                            39404 non-null  float64\n",
            " 1   campaign                       39404 non-null  float64\n",
            " 2   pdays                          39404 non-null  float64\n",
            " 3   previous                       39404 non-null  float64\n",
            " 4   poutcome                       39404 non-null  float64\n",
            " 5   emp.var.rate                   39404 non-null  float64\n",
            " 6   cons.price.idx                 39404 non-null  float64\n",
            " 7   cons.conf.idx                  39404 non-null  float64\n",
            " 8   euribor3m                      39404 non-null  float64\n",
            " 9   nr.employed                    39404 non-null  float64\n",
            " 10  y                              39404 non-null  int64  \n",
            " 11  job_blue-collar                39404 non-null  float64\n",
            " 12  job_entrepreneur               39404 non-null  float64\n",
            " 13  job_housemaid                  39404 non-null  float64\n",
            " 14  job_management                 39404 non-null  float64\n",
            " 15  job_retired                    39404 non-null  float64\n",
            " 16  job_self-employed              39404 non-null  float64\n",
            " 17  job_services                   39404 non-null  float64\n",
            " 18  job_student                    39404 non-null  float64\n",
            " 19  job_technician                 39404 non-null  float64\n",
            " 20  job_unemployed                 39404 non-null  float64\n",
            " 21  job_unknown                    39404 non-null  float64\n",
            " 22  marital_married                39404 non-null  float64\n",
            " 23  marital_single                 39404 non-null  float64\n",
            " 24  marital_unknown                39404 non-null  float64\n",
            " 25  education_basic.6y             39404 non-null  float64\n",
            " 26  education_basic.9y             39404 non-null  float64\n",
            " 27  education_high.school          39404 non-null  float64\n",
            " 28  education_illiterate           39404 non-null  float64\n",
            " 29  education_professional.course  39404 non-null  float64\n",
            " 30  education_university.degree    39404 non-null  float64\n",
            " 31  education_unknown              39404 non-null  float64\n",
            " 32  default_unknown                39404 non-null  float64\n",
            " 33  default_yes                    39404 non-null  float64\n",
            " 34  housing_unknown                39404 non-null  float64\n",
            " 35  housing_yes                    39404 non-null  float64\n",
            " 36  loan_unknown                   39404 non-null  float64\n",
            " 37  loan_yes                       39404 non-null  float64\n",
            " 38  contact_telephone              39404 non-null  float64\n",
            " 39  month_aug                      39404 non-null  float64\n",
            " 40  month_dec                      39404 non-null  float64\n",
            " 41  month_jul                      39404 non-null  float64\n",
            " 42  month_jun                      39404 non-null  float64\n",
            " 43  month_mar                      39404 non-null  float64\n",
            " 44  month_may                      39404 non-null  float64\n",
            " 45  month_nov                      39404 non-null  float64\n",
            " 46  month_oct                      39404 non-null  float64\n",
            " 47  month_sep                      39404 non-null  float64\n",
            " 48  day_of_week_mon                39404 non-null  float64\n",
            " 49  day_of_week_thu                39404 non-null  float64\n",
            " 50  day_of_week_tue                39404 non-null  float64\n",
            " 51  day_of_week_wed                39404 non-null  float64\n",
            "dtypes: float64(51), int64(1)\n",
            "memory usage: 15.9 MB\n",
            "\n",
            "Final DataFrame Preview:\n",
            "        age  campaign  pdays  previous  poutcome  emp.var.rate  \\\n",
            "0  0.481481       0.0    1.0       0.0       0.5        0.9375   \n",
            "1  0.493827       0.0    1.0       0.0       0.5        0.9375   \n",
            "2  0.246914       0.0    1.0       0.0       0.5        0.9375   \n",
            "3  0.283951       0.0    1.0       0.0       0.5        0.9375   \n",
            "4  0.481481       0.0    1.0       0.0       0.5        0.9375   \n",
            "\n",
            "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed  ...  month_jun  \\\n",
            "0        0.698753        0.60251   0.957379     0.859735  ...        0.0   \n",
            "1        0.698753        0.60251   0.957379     0.859735  ...        0.0   \n",
            "2        0.698753        0.60251   0.957379     0.859735  ...        0.0   \n",
            "3        0.698753        0.60251   0.957379     0.859735  ...        0.0   \n",
            "4        0.698753        0.60251   0.957379     0.859735  ...        0.0   \n",
            "\n",
            "   month_mar  month_may  month_nov  month_oct  month_sep  day_of_week_mon  \\\n",
            "0        0.0        1.0        0.0        0.0        0.0              1.0   \n",
            "1        0.0        1.0        0.0        0.0        0.0              1.0   \n",
            "2        0.0        1.0        0.0        0.0        0.0              1.0   \n",
            "3        0.0        1.0        0.0        0.0        0.0              1.0   \n",
            "4        0.0        1.0        0.0        0.0        0.0              1.0   \n",
            "\n",
            "   day_of_week_thu  day_of_week_tue  day_of_week_wed  \n",
            "0              0.0              0.0              0.0  \n",
            "1              0.0              0.0              0.0  \n",
            "2              0.0              0.0              0.0  \n",
            "3              0.0              0.0              0.0  \n",
            "4              0.0              0.0              0.0  \n",
            "\n",
            "[5 rows x 52 columns]\n",
            "Class Distribution (%):\n",
            "y\n",
            "0    88.331134\n",
            "1    11.668866\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set output directory in Google Drive\n",
        "output_dir = '/content/drive/My Drive/ML_CW/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Define file paths\n",
        "transformed_path = os.path.join(output_dir, 'transformed_add.csv')\n",
        "train_data_path = os.path.join(output_dir, 'train_data_add.csv')\n",
        "test_data_path = os.path.join(output_dir, 'test_data_add.csv')\n",
        "\n",
        "try:\n",
        "    # Save datasets\n",
        "    data_add.to_csv(transformed_path, index=False)\n",
        "    train_data_add.to_csv(train_data_path, index=False)\n",
        "    test_data_add.to_csv(test_data_path, index=False)\n",
        "\n",
        "    # Confirm saved files\n",
        "    if os.path.exists(transformed_path):\n",
        "        print(f\"Transformed data saved to: {transformed_path}\")\n",
        "    else:\n",
        "        print(\"Error: Transformed data not saved.\")\n",
        "\n",
        "    if os.path.exists(train_data_path):\n",
        "        print(f\"Training data saved to: {train_data_path}\")\n",
        "    else:\n",
        "        print(\"Error: Training data not saved.\")\n",
        "\n",
        "    if os.path.exists(test_data_path):\n",
        "        print(f\"Testing data saved to: {test_data_path}\")\n",
        "    else:\n",
        "        print(\"Error: Testing data not saved.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error while saving files: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lQR5EcPdZjt",
        "outputId": "79bb8e5f-90dc-47f0-a868-1da955a8b2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Transformed data saved to: /content/drive/My Drive/ML_CW/transformed_add.csv\n",
            "Training data saved to: /content/drive/My Drive/ML_CW/train_data_add.csv\n",
            "Testing data saved to: /content/drive/My Drive/ML_CW/test_data_add.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# File paths for training and testing data\n",
        "train_file_path = '/content/drive/My Drive/ML_CW/train_data_add.csv'\n",
        "test_file_path = '/content/drive/My Drive/ML_CW/test_data_add.csv'\n",
        "\n",
        "# Load training and testing data\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "X_train = train_data.drop('y', axis=1)\n",
        "y_train = train_data['y']\n",
        "\n",
        "X_test = test_data.drop('y', axis=1)\n",
        "y_test = test_data['y']\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# fit the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# Output to debug\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Feature Importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf_classifier.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop Features:\")\n",
        "print(feature_importances.head())\n"
      ],
      "metadata": {
        "id": "Qn6yYVk_AQJl",
        "outputId": "887c9023-5e4b-4138-fe00-7d6ba48f6bd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94      6961\n",
            "           1       0.51      0.36      0.42       920\n",
            "\n",
            "    accuracy                           0.88      7881\n",
            "   macro avg       0.71      0.66      0.68      7881\n",
            "weighted avg       0.87      0.88      0.88      7881\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[6640  321]\n",
            " [ 587  333]]\n",
            "\n",
            "Top Features:\n",
            "        Feature  Importance\n",
            "8     euribor3m    0.127059\n",
            "1      campaign    0.113511\n",
            "9   nr.employed    0.086621\n",
            "0           age    0.080504\n",
            "5  emp.var.rate    0.050869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define file paths in Google Drive\n",
        "train_data_path = '/content/drive/My Drive/ML_CW/train_data_add.csv'\n",
        "test_data_path = '/content/drive/My Drive/ML_CW/test_data_add.csv'\n",
        "\n",
        "# Load the datasets\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "# Split features (X) and target (y)\n",
        "X_train = train_data.drop('y', axis=1)\n",
        "y_train = train_data['y']\n",
        "X_test = test_data.drop('y', axis=1)\n",
        "y_test = test_data['y']\n",
        "\n",
        "# Build the Neural Network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_dim=X_train.shape[1]),  # Input layer\n",
        "    Dropout(0.3),  # Regularization\n",
        "    Dense(32, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "model_save_path = '/content/drive/My Drive/ML_CW/bank_marketing_model.h5'\n",
        "model.save(model_save_path)\n",
        "print(f\"\\nModel saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "IxW0s88lx4VK",
        "outputId": "7e2761f3-1696-4713-81d2-3557e92d3294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7274 - loss: 0.5641 - val_accuracy: 0.5617 - val_loss: 0.7368\n",
            "Epoch 2/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.5076 - val_accuracy: 0.5962 - val_loss: 0.6920\n",
            "Epoch 3/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7797 - loss: 0.4974 - val_accuracy: 0.5892 - val_loss: 0.6911\n",
            "Epoch 4/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7849 - loss: 0.4847 - val_accuracy: 0.6152 - val_loss: 0.6776\n",
            "Epoch 5/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7844 - loss: 0.4785 - val_accuracy: 0.5864 - val_loss: 0.7379\n",
            "Epoch 6/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7891 - loss: 0.4697 - val_accuracy: 0.6099 - val_loss: 0.6705\n",
            "Epoch 7/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7905 - loss: 0.4630 - val_accuracy: 0.6259 - val_loss: 0.6432\n",
            "Epoch 8/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7942 - loss: 0.4591 - val_accuracy: 0.6243 - val_loss: 0.6272\n",
            "Epoch 9/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7954 - loss: 0.4532 - val_accuracy: 0.6439 - val_loss: 0.5798\n",
            "Epoch 10/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7926 - loss: 0.4519 - val_accuracy: 0.6278 - val_loss: 0.6048\n",
            "Epoch 11/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7970 - loss: 0.4470 - val_accuracy: 0.6610 - val_loss: 0.5517\n",
            "Epoch 12/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7994 - loss: 0.4458 - val_accuracy: 0.6134 - val_loss: 0.6779\n",
            "Epoch 13/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7990 - loss: 0.4423 - val_accuracy: 0.6659 - val_loss: 0.5705\n",
            "Epoch 14/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7999 - loss: 0.4372 - val_accuracy: 0.6428 - val_loss: 0.6071\n",
            "Epoch 15/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8046 - loss: 0.4329 - val_accuracy: 0.6461 - val_loss: 0.5782\n",
            "Epoch 16/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8061 - loss: 0.4305 - val_accuracy: 0.6572 - val_loss: 0.5708\n",
            "Epoch 17/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8043 - loss: 0.4318 - val_accuracy: 0.6444 - val_loss: 0.5814\n",
            "Epoch 18/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8114 - loss: 0.4218 - val_accuracy: 0.6368 - val_loss: 0.6126\n",
            "Epoch 19/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8114 - loss: 0.4229 - val_accuracy: 0.6698 - val_loss: 0.5678\n",
            "Epoch 20/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8065 - loss: 0.4266 - val_accuracy: 0.6429 - val_loss: 0.6179\n",
            "Epoch 21/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8108 - loss: 0.4194 - val_accuracy: 0.6636 - val_loss: 0.5669\n",
            "Epoch 22/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8086 - loss: 0.4219 - val_accuracy: 0.6783 - val_loss: 0.5417\n",
            "Epoch 23/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.4185 - val_accuracy: 0.6699 - val_loss: 0.5613\n",
            "Epoch 24/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8156 - loss: 0.4150 - val_accuracy: 0.6566 - val_loss: 0.5843\n",
            "Epoch 25/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8116 - loss: 0.4168 - val_accuracy: 0.6905 - val_loss: 0.5289\n",
            "Epoch 26/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8149 - loss: 0.4114 - val_accuracy: 0.6757 - val_loss: 0.5641\n",
            "Epoch 27/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8160 - loss: 0.4128 - val_accuracy: 0.6740 - val_loss: 0.5649\n",
            "Epoch 28/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8141 - loss: 0.4140 - val_accuracy: 0.6990 - val_loss: 0.5160\n",
            "Epoch 29/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8221 - loss: 0.4111 - val_accuracy: 0.7015 - val_loss: 0.5134\n",
            "Epoch 30/30\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8158 - loss: 0.4119 - val_accuracy: 0.6674 - val_loss: 0.5864\n",
            "\n",
            "Test Accuracy: 0.8716\n",
            "\u001b[1m247/247\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      6961\n",
            "           1       0.45      0.45      0.45       920\n",
            "\n",
            "    accuracy                           0.87      7881\n",
            "   macro avg       0.69      0.69      0.69      7881\n",
            "weighted avg       0.87      0.87      0.87      7881\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[6452  509]\n",
            " [ 503  417]]\n",
            "\n",
            "Model saved to: /content/drive/My Drive/ML_CW/bank_marketing_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bYNdIYyFf2Qv"
      }
    }
  ]
}