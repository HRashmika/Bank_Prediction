{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HRashmika/Bank_Prediction/blob/main/ML_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LZz5aUWV3KHu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMnmY_xPdkT",
        "outputId": "8d99c51a-06f5-4ddf-ad16-4884ddd3b581"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'bank.csv'\n",
        "data_bank = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# For debugging\n",
        "print(\"Original DataFrame:\")\n",
        "print(data_bank.head())\n",
        "\n",
        "# After performing one-hot encoding, keep 'y' separate for splitting:\n",
        "# One-hot encoding (without dropping 'y')\n",
        "binary_columns = ['default', 'housing', 'loan']  # Remove 'y' from binary_columns\n",
        "data_bank = pd.get_dummies(data_bank, columns=binary_columns, drop_first=True)\n",
        "\n",
        "# O and 1 for binary columns\n",
        "binary_cols = [col for col in data_bank.columns if data_bank[col].dtype == 'bool']\n",
        "for col in binary_cols:\n",
        "    data_bank[col] = data_bank[col].astype(int)\n",
        "\n",
        "# Label encoding\n",
        "label_columns = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']\n",
        "label_encoder = LabelEncoder()\n",
        "label_mappings = {}\n",
        "\n",
        "for col in label_columns:\n",
        "    data_bank[col] = label_encoder.fit_transform(data_bank[col])\n",
        "    label_mappings[col] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "print(\"\\nLabel Mappings:\")\n",
        "for col, mapping in label_mappings.items():\n",
        "    print(f\"{col}: {mapping}\")\n",
        "\n",
        "print(\"\\nTransformed DataFrame:\")\n",
        "print(data_bank.head())\n",
        "\n",
        "# Duplicates\n",
        "duplicates = data_bank[data_bank.duplicated()]\n",
        "if not duplicates.empty:\n",
        "    print(\"\\nDuplicate Rows Found:\")\n",
        "    print(duplicates)\n",
        "    data_bank = data_bank.drop_duplicates()\n",
        "    print(\"\\nDuplicates removed. Current shape of DataFrame:\", data_bank.shape)\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "# Missing values\n",
        "missing_values = data_bank.isnull().sum()\n",
        "if missing_values.any():\n",
        "    print(\"\\nMissing Values Found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "\n",
        "    # Replace all missing values with the string 'non'\n",
        "    for col in data_bank.columns:\n",
        "        data_bank[col].fillna(\"non\", inplace=True)\n",
        "else:\n",
        "    print(\"\\nNo missing values found.\")\n",
        "\n",
        "# Valid or invalid, checked with label encoding\n",
        "columns_to_check = ['job', 'marital', 'education', 'contact', 'poutcome']\n",
        "for col in columns_to_check:\n",
        "    max_label = len(label_mappings[col]) - 1\n",
        "    invalid_values = data_bank[col][(data_bank[col] < 0) | (data_bank[col] > max_label)].unique()\n",
        "    if invalid_values.size > 0:\n",
        "        print(f\"\\nInvalid values in column '{col}': {invalid_values}\")\n",
        "        data_bank = data_bank[~data_bank[col].isin(invalid_values)]\n",
        "    else:\n",
        "        print(f\"\\nNo invalid values found in column '{col}'.\")\n",
        "\n",
        "\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "X = data_bank.drop('y', axis=1, errors='ignore')  # Ensure 'y' is ignored if it was dropped\n",
        "y = data_bank.get('y')  # Extract 'y' from the DataFrame\n",
        "\n",
        "if y is not None:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    train_data_bank = X_train.copy()\n",
        "    train_data_bank['y'] = y_train\n",
        "\n",
        "    test_data_bank = X_test.copy()\n",
        "    test_data_bank['y'] = y_test\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"\\n'Y' column is missing, cannot split data.\")\n",
        "\n",
        "print(\"\\nFinal DataFrame Info:\")\n",
        "data_bank.info()\n",
        "print(\"\\nFinal DataFrame Preview:\")\n",
        "print(data_bank.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7Czl4oH83YrX",
        "outputId": "0b072ce4-9f8e-4514-dee5-30e692b66747"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'bank.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0fbf1021b529>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bank.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_bank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original DataFrame:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bank.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# File path\n",
        "file_path = 'bank-additional-full.csv'\n",
        "data_add = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Debugging: print the original data\n",
        "print(\"Original DataFrame:\")\n",
        "print(data_add.head())\n",
        "\n",
        "# Drop the 'duration' column\n",
        "data_add = data_add.drop('duration', axis=1, errors='ignore')  # Drop column if it exists\n",
        "\n",
        "# One-hot encoding for binary columns (encoding 'yes'/'no' columns as 1 and 0)\n",
        "binary_columns = ['default', 'housing', 'loan']\n",
        "data_add = pd.get_dummies(data_add, columns=binary_columns, drop_first=True)\n",
        "\n",
        "# Convert boolean columns (if any) to integers\n",
        "binary_cols = [col for col in data_add.columns if data_add[col].dtype == 'bool']\n",
        "for col in binary_cols:\n",
        "    data_add[col] = data_add[col].astype(int)\n",
        "\n",
        "# Label encoding for categorical columns (job, marital, education, etc.)\n",
        "label_columns = ['job', 'marital', 'education', 'contact', 'month', 'day_of_week', 'poutcome']\n",
        "label_encoder = LabelEncoder()\n",
        "label_mappings = {}\n",
        "\n",
        "for col in label_columns:\n",
        "    data_add[col] = label_encoder.fit_transform(data_add[col].fillna('unknown'))  # Filling NaNs with 'unknown' before encoding\n",
        "    label_mappings[col] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "# Print label mappings for reference\n",
        "print(\"\\nLabel Mappings:\")\n",
        "for col, mapping in label_mappings.items():\n",
        "    print(f\"{col}: {mapping}\")\n",
        "\n",
        "# Encode 'y' column as 1 for 'yes' and 0 for 'no'\n",
        "data_add['y'] = data_add['y'].map({'yes': 1, 'no': 0})\n",
        "\n",
        "# Check and handle duplicates (rows where all values are identical)\n",
        "duplicates = data_add[data_add.duplicated()]\n",
        "if not duplicates.empty:\n",
        "    print(\"\\nDuplicate Rows Found:\")\n",
        "    print(duplicates)\n",
        "    data_add = data_add.drop_duplicates()  # Remove duplicates\n",
        "    print(\"\\nDuplicates removed. Current shape of DataFrame:\", data_add.shape)\n",
        "else:\n",
        "    print(\"\\nNo duplicate rows found.\")\n",
        "\n",
        "# Check and handle missing values\n",
        "missing_values = data_add.isnull().sum()\n",
        "if missing_values.any():\n",
        "    print(\"\\nMissing Values Found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "    # Fill missing values with 'unknown' for categorical and 'non' for other columns\n",
        "    for col in data_add.columns:\n",
        "        if data_add[col].dtype == 'object':\n",
        "            data_add[col].fillna('unknown', inplace=True)\n",
        "        else:\n",
        "            data_add[col].fillna('non', inplace=True)\n",
        "else:\n",
        "    print(\"\\nNo missing values found.\")\n",
        "\n",
        "# Check for invalid values in label-encoded columns\n",
        "columns_to_check = label_columns\n",
        "for col in columns_to_check:\n",
        "    max_label = len(label_mappings[col]) - 1\n",
        "    invalid_values = data_add[col][(data_add[col] < 0) | (data_add[col] > max_label)].unique()\n",
        "    if invalid_values.size > 0:\n",
        "        print(f\"\\nInvalid values in column '{col}': {invalid_values}\")\n",
        "        data_add = data_add[~data_add[col].isin(invalid_values)]  # Remove rows with invalid values\n",
        "    else:\n",
        "        print(f\"\\nNo invalid values found in column '{col}'.\")\n",
        "\n",
        "# Min-Max Normalization (scaling the features to the range [0, 1])\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling only to numeric features (excluding the target 'y')\n",
        "features_to_scale = data_add.drop('y', axis=1)  # Excluding 'y' from scaling\n",
        "data_add[features_to_scale.columns] = scaler.fit_transform(features_to_scale)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data_add.drop('y', axis=1, errors='ignore')  # Ensure 'y' is ignored if it was dropped\n",
        "y = data_add.get('y')  # Extract 'y' from the DataFrame\n",
        "\n",
        "# Perform train-test split if the target column 'y' exists\n",
        "if y is not None:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Combine X and y for training and testing datasets\n",
        "    train_data_add = X_train.copy()\n",
        "    train_data_add['y'] = y_train\n",
        "\n",
        "    test_data_add = X_test.copy()\n",
        "    test_data_add['y'] = y_test\n",
        "\n",
        "    print(\"\\nTraining Data Shape:\", train_data_add.shape)\n",
        "    print(\"Testing Data Shape:\", test_data_add.shape)\n",
        "else:\n",
        "    print(\"\\n'Y' column is missing, cannot split data.\")\n",
        "\n",
        "# Print final DataFrame information\n",
        "print(\"\\nFinal DataFrame Info:\")\n",
        "data_add.info()\n",
        "print(\"\\nFinal DataFrame Preview:\")\n",
        "print(data_add.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efsQjCUFXYHN",
        "outputId": "d7fd53b6-5f56-4d63-8aa5-175051a1f203"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "   age        job  marital    education  default housing loan    contact  \\\n",
            "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
            "1   57   services  married  high.school  unknown      no   no  telephone   \n",
            "2   37   services  married  high.school       no     yes   no  telephone   \n",
            "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
            "4   56   services  married  high.school       no      no  yes  telephone   \n",
            "\n",
            "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
            "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
            "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
            "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
            "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
            "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
            "\n",
            "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
            "0          93.994          -36.4      4.857       5191.0  no  \n",
            "1          93.994          -36.4      4.857       5191.0  no  \n",
            "2          93.994          -36.4      4.857       5191.0  no  \n",
            "3          93.994          -36.4      4.857       5191.0  no  \n",
            "4          93.994          -36.4      4.857       5191.0  no  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "Label Mappings:\n",
            "job: {'admin.': 0, 'blue-collar': 1, 'entrepreneur': 2, 'housemaid': 3, 'management': 4, 'retired': 5, 'self-employed': 6, 'services': 7, 'student': 8, 'technician': 9, 'unemployed': 10, 'unknown': 11}\n",
            "marital: {'divorced': 0, 'married': 1, 'single': 2, 'unknown': 3}\n",
            "education: {'basic.4y': 0, 'basic.6y': 1, 'basic.9y': 2, 'high.school': 3, 'illiterate': 4, 'professional.course': 5, 'university.degree': 6, 'unknown': 7}\n",
            "contact: {'cellular': 0, 'telephone': 1}\n",
            "month: {'apr': 0, 'aug': 1, 'dec': 2, 'jul': 3, 'jun': 4, 'mar': 5, 'may': 6, 'nov': 7, 'oct': 8, 'sep': 9}\n",
            "day_of_week: {'fri': 0, 'mon': 1, 'thu': 2, 'tue': 3, 'wed': 4}\n",
            "poutcome: {'failure': 0, 'nonexistent': 1, 'success': 2}\n",
            "\n",
            "Duplicate Rows Found:\n",
            "       age  job  marital  education  contact  month  day_of_week  campaign  \\\n",
            "10      41    1        1          7        1      6            1         1   \n",
            "11      25    7        2          3        1      6            1         1   \n",
            "16      35    1        1          1        1      6            1         1   \n",
            "31      59    9        1          7        1      6            1         1   \n",
            "104     52    0        0          6        1      6            1         1   \n",
            "...    ...  ...      ...        ...      ...    ...          ...       ...   \n",
            "39985   27    0        2          3        0      4            3         2   \n",
            "40401   31    8        2          7        0      1            2         2   \n",
            "40404   41    2        1          6        0      1            2         1   \n",
            "40806   35    9        1          5        0      9            2         1   \n",
            "40840   32    0        2          6        0      9            1         4   \n",
            "\n",
            "       pdays  previous  ...  cons.conf.idx  euribor3m  nr.employed  y  \\\n",
            "10       999         0  ...          -36.4      4.857       5191.0  0   \n",
            "11       999         0  ...          -36.4      4.857       5191.0  0   \n",
            "16       999         0  ...          -36.4      4.857       5191.0  0   \n",
            "31       999         0  ...          -36.4      4.857       5191.0  0   \n",
            "104      999         0  ...          -36.4      4.857       5191.0  0   \n",
            "...      ...       ...  ...            ...        ...          ... ..   \n",
            "39985    999         0  ...          -39.8      0.761       4991.6  1   \n",
            "40401    999         0  ...          -38.3      0.904       4991.6  1   \n",
            "40404    999         0  ...          -38.3      0.904       4991.6  1   \n",
            "40806    999         2  ...          -37.5      0.878       4963.6  0   \n",
            "40840    999         0  ...          -37.5      0.879       4963.6  0   \n",
            "\n",
            "       default_unknown  default_yes  housing_unknown  housing_yes  \\\n",
            "10                   1            0                0            0   \n",
            "11                   0            0                0            1   \n",
            "16                   0            0                0            1   \n",
            "31                   0            0                0            1   \n",
            "104                  0            0                0            0   \n",
            "...                ...          ...              ...          ...   \n",
            "39985                0            0                0            0   \n",
            "40401                0            0                0            1   \n",
            "40404                0            0                0            1   \n",
            "40806                0            0                0            1   \n",
            "40840                0            0                0            1   \n",
            "\n",
            "       loan_unknown  loan_yes  \n",
            "10                0         0  \n",
            "11                0         0  \n",
            "16                0         0  \n",
            "31                0         0  \n",
            "104               0         0  \n",
            "...             ...       ...  \n",
            "39985             0         0  \n",
            "40401             0         0  \n",
            "40404             0         0  \n",
            "40806             0         0  \n",
            "40840             0         0  \n",
            "\n",
            "[1784 rows x 23 columns]\n",
            "\n",
            "Duplicates removed. Current shape of DataFrame: (39404, 23)\n",
            "\n",
            "No missing values found.\n",
            "\n",
            "No invalid values found in column 'job'.\n",
            "\n",
            "No invalid values found in column 'marital'.\n",
            "\n",
            "No invalid values found in column 'education'.\n",
            "\n",
            "No invalid values found in column 'contact'.\n",
            "\n",
            "No invalid values found in column 'month'.\n",
            "\n",
            "No invalid values found in column 'day_of_week'.\n",
            "\n",
            "No invalid values found in column 'poutcome'.\n",
            "\n",
            "Training Data Shape: (31523, 23)\n",
            "Testing Data Shape: (7881, 23)\n",
            "\n",
            "Final DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 39404 entries, 0 to 41187\n",
            "Data columns (total 23 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   age              39404 non-null  float64\n",
            " 1   job              39404 non-null  float64\n",
            " 2   marital          39404 non-null  float64\n",
            " 3   education        39404 non-null  float64\n",
            " 4   contact          39404 non-null  float64\n",
            " 5   month            39404 non-null  float64\n",
            " 6   day_of_week      39404 non-null  float64\n",
            " 7   campaign         39404 non-null  float64\n",
            " 8   pdays            39404 non-null  float64\n",
            " 9   previous         39404 non-null  float64\n",
            " 10  poutcome         39404 non-null  float64\n",
            " 11  emp.var.rate     39404 non-null  float64\n",
            " 12  cons.price.idx   39404 non-null  float64\n",
            " 13  cons.conf.idx    39404 non-null  float64\n",
            " 14  euribor3m        39404 non-null  float64\n",
            " 15  nr.employed      39404 non-null  float64\n",
            " 16  y                39404 non-null  int64  \n",
            " 17  default_unknown  39404 non-null  float64\n",
            " 18  default_yes      39404 non-null  float64\n",
            " 19  housing_unknown  39404 non-null  float64\n",
            " 20  housing_yes      39404 non-null  float64\n",
            " 21  loan_unknown     39404 non-null  float64\n",
            " 22  loan_yes         39404 non-null  float64\n",
            "dtypes: float64(22), int64(1)\n",
            "memory usage: 7.2 MB\n",
            "\n",
            "Final DataFrame Preview:\n",
            "        age       job   marital  education  contact     month  day_of_week  \\\n",
            "0  0.481481  0.272727  0.333333   0.000000      1.0  0.666667         0.25   \n",
            "1  0.493827  0.636364  0.333333   0.428571      1.0  0.666667         0.25   \n",
            "2  0.246914  0.636364  0.333333   0.428571      1.0  0.666667         0.25   \n",
            "3  0.283951  0.000000  0.333333   0.142857      1.0  0.666667         0.25   \n",
            "4  0.481481  0.636364  0.333333   0.428571      1.0  0.666667         0.25   \n",
            "\n",
            "   campaign  pdays  previous  ...  cons.conf.idx  euribor3m  nr.employed  y  \\\n",
            "0       0.0    1.0       0.0  ...        0.60251   0.957379     0.859735  0   \n",
            "1       0.0    1.0       0.0  ...        0.60251   0.957379     0.859735  0   \n",
            "2       0.0    1.0       0.0  ...        0.60251   0.957379     0.859735  0   \n",
            "3       0.0    1.0       0.0  ...        0.60251   0.957379     0.859735  0   \n",
            "4       0.0    1.0       0.0  ...        0.60251   0.957379     0.859735  0   \n",
            "\n",
            "   default_unknown  default_yes  housing_unknown  housing_yes  loan_unknown  \\\n",
            "0              0.0          0.0              0.0          0.0           0.0   \n",
            "1              1.0          0.0              0.0          0.0           0.0   \n",
            "2              0.0          0.0              0.0          1.0           0.0   \n",
            "3              0.0          0.0              0.0          0.0           0.0   \n",
            "4              0.0          0.0              0.0          0.0           0.0   \n",
            "\n",
            "   loan_yes  \n",
            "0       0.0  \n",
            "1       0.0  \n",
            "2       0.0  \n",
            "3       0.0  \n",
            "4       1.0  \n",
            "\n",
            "[5 rows x 23 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-b2cada13de8a>:82: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_add[features_to_scale.columns] = scaler.fit_transform(features_to_scale)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_path = '/content/drive/My Drive/ML_CW/transformed_add.csv'\n",
        "data_add.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Transformed data saved to: {output_path}\")\n",
        "\n",
        "train_data_add.to_csv('/content/drive/My Drive/ML_CW/train_data_add.csv', index=False)\n",
        "test_data_add.to_csv('/content/drive/My Drive/ML_CW/test_data_add.csv', index=False)\n",
        "print(\"\\nTraining and Testing Data Saved:\")\n",
        "print(f\"Training Data: /content/drive/My Drive/ML_CW/train_data_add.csv\")\n",
        "print(f\"Testing Data: /content/drive/My Drive/ML_CW/test_data_add.csv\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lQR5EcPdZjt",
        "outputId": "99ae951c-ad37-4d20-cc8d-c359c1070cc7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data saved to: /content/drive/My Drive/ML_CW/transformed_add.csv\n",
            "\n",
            "Training and Testing Data Saved:\n",
            "Training Data: /content/drive/My Drive/ML_CW/train_data_add.csv\n",
            "Testing Data: /content/drive/My Drive/ML_CW/test_data_add.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bYNdIYyFf2Qv"
      }
    }
  ]
}